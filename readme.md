# ASISTENTE VIRTUAL FLASK PYTHON
paso 1: Descargar e instalar ollama -> https://ollama.com/
paso 2: Escoger el modelo que deseamos usar en github -> https://github.com/ollama/ollama
paso 3: Crear el proyecto donde se va usar el modelo
paso 4: personalizar el modelo en este caso llama3.2 -> ollama pull llama3.2
